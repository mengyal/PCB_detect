import torch
import cv2
import numpy as np
import os
from tqdm import tqdm
import importlib.util
import sys

# é…ç½®å‚æ•°
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
CLASS_COLORS = {
    0: (0, 0, 0),        # background - é»‘è‰²
    1: (0, 255, 0),      # good - ç»¿è‰²
    2: (0, 0, 255),      # insufficient - çº¢è‰²
    3: (0, 165, 255),    # excess - æ©™è‰²
    4: (255, 0, 255),    # shift - ç´«è‰²
    5: (255, 255, 0),    # miss - é»„è‰²
}
OVERLAY_ALPHA = 0.6
MASK_ALPHA = 0.4

def preprocess_image(image, target_size):
    """é¢„å¤„ç†å›¾ç‰‡"""
    original_size = (image.shape[1], image.shape[0])
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    image_resized = cv2.resize(image_rgb, target_size, interpolation=cv2.INTER_LINEAR)
    image_tensor = torch.from_numpy(image_resized.transpose((2, 0, 1))).float() / 255.0
    image_tensor = image_tensor.unsqueeze(0)
    return image_tensor, original_size

def postprocess_prediction(prediction, original_size):
    """åå¤„ç†é¢„æµ‹ç»“æœ"""
    prediction_resized = cv2.resize(
        prediction.astype(np.uint8), 
        original_size, 
        interpolation=cv2.INTER_NEAREST
    )
    return prediction_resized

def create_colored_mask(prediction, class_colors):
    """åˆ›å»ºå½©è‰²åˆ†å‰²æ©ç """
    h, w = prediction.shape
    colored_mask = np.zeros((h, w, 3), dtype=np.uint8)
    for class_id, color in class_colors.items():
        mask = prediction == class_id
        colored_mask[mask] = color
    return colored_mask

def create_overlay_visualization(original_image, colored_mask):
    """åˆ›å»ºå åŠ å¯è§†åŒ–å›¾åƒ"""
    overlay = cv2.addWeighted(original_image, OVERLAY_ALPHA, colored_mask, MASK_ALPHA, 0)
    return overlay


def save_visualizations(overlay, output_dir, base_name):
    """ä¿å­˜å¯è§†åŒ–ç»“æœ"""
    os.makedirs(output_dir, exist_ok=True)
    overlay_path = os.path.join(output_dir, f"{base_name}_overlay.jpg")
    cv2.imwrite(overlay_path, overlay)
    
    # åŒæ—¶ä¿å­˜åˆ°æŠ¥å‘Šç³»ç»Ÿçš„å¯è§†åŒ–ç›®å½•
    report_viz_dir = os.path.join(os.path.dirname(__file__), '..', 'temp', 'report_visualizations')
    os.makedirs(report_viz_dir, exist_ok=True)
    report_overlay_path = os.path.join(report_viz_dir, f"{base_name}_overlay.jpg")
    cv2.imwrite(report_overlay_path, overlay)
    
    print(f"å¯è§†åŒ–ç»“æœä¿å­˜åˆ°: {overlay_path}")
    print(f"æŠ¥å‘Šå¯è§†åŒ–ä¿å­˜åˆ°: {report_overlay_path}")
    return overlay_path, report_overlay_path

def load_model_with_config():
    """åŠ è½½æ¨¡å‹é…ç½®å’Œæƒé‡"""
    print("ğŸ”§ åŠ è½½æ¨¡å‹é…ç½®å’Œæ¶æ„...")

    try:
        # åŠ¨æ€å¯¼å…¥éœ€è¦çš„æ¨¡å—
        unet_path = os.path.join(os.path.dirname(__file__), '..', '2-train', 'unet.py')
        config_path = os.path.join(os.path.dirname(__file__), '..', '2-train', 'model_configs.py')

        # åŠ è½½unetæ¨¡å—
        spec = importlib.util.spec_from_file_location("unet", unet_path)
        unet_module = importlib.util.module_from_spec(spec)
        sys.modules["unet"] = unet_module
        spec.loader.exec_module(unet_module)

        # åŠ è½½é…ç½®æ¨¡å—
        spec = importlib.util.spec_from_file_location("model_configs", config_path)
        config_module = importlib.util.module_from_spec(spec)
        sys.modules["model_configs"] = config_module
        spec.loader.exec_module(config_module)

        print("âœ“ æ¨¡å—åŠ è½½æˆåŠŸ")

        # è·å–ç”¨æˆ·é€‰æ‹©çš„é…ç½®
        config = config_module.get_config(MODEL_CONFIG)
        model_name = config['model_name']
        encoder_name = config['encoder_name']
        encoder_weights = config['encoder_weights']

        print(f"ğŸ“‹ ä½¿ç”¨é…ç½®: {MODEL_CONFIG}")
        print(f"   æ¶æ„: {model_name}")
        print(f"   ç¼–ç å™¨: {encoder_name}")
        print(f"   é¢„è®­ç»ƒæƒé‡: {encoder_weights}")
        print(f"   æè¿°: {config['description']}")

        # æ‰‹åŠ¨åˆ›å»ºæ¨¡å‹
        print(f"ğŸ—ï¸ æ‰‹åŠ¨åˆ›å»ºæ¨¡å‹: {model_name}")
        model = unet_module.create_model(
            n_channels=3,
            n_classes=len(CLASS_COLORS),
            model_name=model_name,
            encoder_name=encoder_name,
            encoder_weights=encoder_weights
        )

        # åŠ è½½æ¨¡å‹æƒé‡
        print(f"ğŸ“¦ åŠ è½½æ¨¡å‹æƒé‡: {MODEL_PATH}")
        checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)
        model.load_state_dict(checkpoint)
        model = model.to(DEVICE)

        return model, config

    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise e

def main():
    """ä¸»ç¨‹åºå…¥å£"""
    print("ğŸ”¬ PCBç„Šç‚¹æ£€æµ‹å¯è§†åŒ–å·¥å…·")
    print("=" * 60)
    print(f"è®¾å¤‡: {DEVICE}")

    # æ£€æŸ¥è·¯å¾„
    if not os.path.exists(INPUT_DIR):
        print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {INPUT_DIR}")
        return

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # è·å–æµ‹è¯•å›¾ç‰‡
    image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff')
    test_images = [f for f in os.listdir(INPUT_DIR) 
                   if f.lower().endswith(image_extensions)]

    if not test_images:
        print(f"âŒ åœ¨ {INPUT_DIR} ä¸­æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶")
        return

    print(f"âœ“ æ‰¾åˆ° {len(test_images)} å¼ å›¾ç‰‡")

    # å¤„ç†å›¾ç‰‡
    for img_name in tqdm(test_images, desc="å¤„ç†è¿›åº¦"):
        try:
            img_path = os.path.join(INPUT_DIR, img_name)
            base_name = os.path.splitext(img_name)[0]

            # è¯»å–å›¾ç‰‡
            original_image = cv2.imread(img_path)
            if original_image is None:
                print(f"âš ï¸ æ— æ³•è¯»å–å›¾ç‰‡: {img_name}")
                continue

            # é¢„å¤„ç†
            input_tensor, original_size = preprocess_image(original_image, TARGET_SIZE)

            # æ¨¡å‹æ¨ç†
            model, config = load_model_with_config()
            model.eval()
            with torch.no_grad():
                prediction = model(input_tensor.to(DEVICE))
                
                # è‡ªåŠ¨æ£€æµ‹è¾“å‡ºé€šé“æ•°å¹¶å¤„ç†
                if len(prediction.shape) == 4 and prediction.shape[1] > 1:
                    # å¤šé€šé“è¾“å‡ºï¼Œä½¿ç”¨argmax
                    prediction = torch.argmax(prediction, dim=1).cpu().numpy()
                else:
                    # å•é€šé“è¾“å‡ºï¼Œç›´æ¥è½¬æ¢
                    prediction = prediction.squeeze().cpu().numpy()

            # è°ƒè¯•æ¨¡å‹è¾“å‡º
            print(f"æ¨¡å‹è¾“å‡ºå½¢çŠ¶: {prediction.shape}")
            print(f"æ¨¡å‹è¾“å‡ºç±»å‹: {type(prediction)}")
            print(f"è¾“å‡ºå€¼èŒƒå›´: {prediction.min()} - {prediction.max()}")

            # ç¡®ä¿predictionæ˜¯äºŒç»´æ•°ç»„
            if len(prediction.shape) > 2:
                prediction = prediction[0]  # å–ç¬¬ä¸€ä¸ªæ‰¹æ¬¡
            
            # åå¤„ç†
            prediction = postprocess_prediction(prediction, original_size)

            # åˆ›å»ºå¯è§†åŒ–
            colored_mask = create_colored_mask(prediction, CLASS_COLORS)
            overlay = create_overlay_visualization(original_image, colored_mask)

            # ä¿å­˜ç»“æœ
            overlay_path, report_overlay_path = save_visualizations(overlay, OUTPUT_DIR, base_name)

        except Exception as e:
            print(f"âš ï¸ å¤„ç†å›¾ç‰‡ {img_name} æ—¶å‡ºé”™: {e}")
            continue

    print(f"\nâœ… å¯è§†åŒ–å®Œæˆ!")
    print(f"ğŸ“‚ ç»“æœä¿å­˜åœ¨: {OUTPUT_DIR}")

if __name__ == "__main__":
    INPUT_DIR = "../../data/users"  # æ›¿æ¢ä¸ºå®é™…è¾“å…¥ç›®å½•è·¯å¾„
    OUTPUT_DIR = "../../temp/visualization"  # æ›¿æ¢ä¸ºå®é™…è¾“å‡ºç›®å½•è·¯å¾„
    TARGET_SIZE = (512, 512)  # æ¨¡å‹è®­ç»ƒæ—¶çš„è¾“å…¥å°ºå¯¸
    MODEL_CONFIG = "pcb_optimized"  # é»˜è®¤æ¨¡å‹é…ç½®
    MODEL_PATH = "../../models/trained/unetpp_solder_model.pth"
    main()
