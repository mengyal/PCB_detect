import torch
import cv2
import numpy as np
import os
from tqdm import tqdm
import sys
from datetime import datetime
import importlib.util

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥è®­ç»ƒæ¨¡å—
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
train_dir = os.path.join(parent_dir, '2-train')
project_root = os.path.dirname(parent_dir)
sys.path.insert(0, train_dir)

# --- é…ç½®å‚æ•° ---
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
MODEL_PATH = os.path.join(project_root, 'models', 'trained', 'unetpp_solder_model.pth')
INPUT_DIR = os.path.join(project_root, 'data', 'users')
OUTPUT_DIR = os.path.join(project_root, 'temp1','plus')
TARGET_SIZE = (512, 512)  # æ¨¡å‹è®­ç»ƒæ—¶çš„è¾“å…¥å°ºå¯¸

# ä½¿ç”¨pcb_optimizedé…ç½®
MODEL_CONFIG = 'pcb_optimized' # è¿™é‡Œå¯ä»¥æ›¿æ¢ä¸ºå…¶ä»–é…ç½®ï¼Œå¦‚ 'recommended', 'basic', ç­‰

# ç±»åˆ«æ˜ å°„ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
CLASS_MAPPING = {
    'background': 0,
    'good': 1,
    'insufficient': 2,
    'excess': 3,
    'shift': 4,
    'miss': 5,
}

ID_TO_CLASS = {v: k for k, v in CLASS_MAPPING.items()}
NUM_CLASSES = len(CLASS_MAPPING)

# ç±»åˆ«é¢œè‰² (BGRæ ¼å¼)
CLASS_COLORS = {
    0: (0, 0, 0),        # background - é»‘è‰²
    1: (0, 255, 0),      # good - ç»¿è‰²
    2: (0, 0, 255),      # insufficient - çº¢è‰²
    3: (0, 165, 255),    # excess - æ©™è‰²
    4: (255, 0, 255),    # shift - ç´«è‰²
    5: (255, 255, 0),    # miss - é»„è‰²
}

# é€æ˜åº¦è®¾ç½®
OVERLAY_ALPHA = 0.6
MASK_ALPHA = 0.4

def load_model_with_config():
    """æ‰‹åŠ¨åŠ è½½æ¨¡å‹"""
    print(f"ğŸ”§ åŠ è½½æ¨¡å‹é…ç½®å’Œæ¶æ„...")

    try:
        # åŠ¨æ€å¯¼å…¥éœ€è¦çš„æ¨¡å—
        unet_path = os.path.join(train_dir, 'unet.py')
        config_path = os.path.join(train_dir, 'model_configs.py')

        # åŠ è½½unetæ¨¡å—
        spec = importlib.util.spec_from_file_location("unet", unet_path)
        unet_module = importlib.util.module_from_spec(spec)
        sys.modules["unet"] = unet_module
        spec.loader.exec_module(unet_module)

        # åŠ è½½é…ç½®æ¨¡å—
        spec = importlib.util.spec_from_file_location("model_configs", config_path)
        config_module = importlib.util.module_from_spec(spec)
        sys.modules["model_configs"] = config_module
        spec.loader.exec_module(config_module)

        print("âœ“ æ¨¡å—åŠ è½½æˆåŠŸ")

        # è·å–ç”¨æˆ·é€‰æ‹©çš„é…ç½®
        config = config_module.get_config(MODEL_CONFIG)
        model_name = config['model_name']
        encoder_name = config['encoder_name']
        encoder_weights = config['encoder_weights']

        print(f"ğŸ“‹ ä½¿ç”¨é…ç½®: {MODEL_CONFIG}")
        print(f"   æ¶æ„: {model_name}")
        print(f"   ç¼–ç å™¨: {encoder_name}")
        print(f"   é¢„è®­ç»ƒæƒé‡: {encoder_weights}")
        print(f"   æè¿°: {config['description']}")

        # æ‰‹åŠ¨åˆ›å»ºæ¨¡å‹
        print(f"ğŸ—ï¸ æ‰‹åŠ¨åˆ›å»ºæ¨¡å‹: {model_name}")
        model = unet_module.create_model(
            n_channels=3,
            n_classes=NUM_CLASSES,
            model_name=model_name,
            encoder_name=encoder_name,
            encoder_weights=encoder_weights
        )

        return model, config

    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise e

def preprocess_image(image, target_size):
    """é¢„å¤„ç†å›¾ç‰‡"""
    # ä¿å­˜åŸå§‹å°ºå¯¸
    original_size = (image.shape[1], image.shape[0])
    
    # è½¬æ¢é¢œè‰²ç©ºé—´
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    
    # è°ƒæ•´å°ºå¯¸
    image_resized = cv2.resize(image_rgb, target_size, interpolation=cv2.INTER_LINEAR)
    
    # è½¬æ¢ä¸ºtensorå¹¶å½’ä¸€åŒ–
    image_tensor = torch.from_numpy(image_resized.transpose((2, 0, 1))).float() / 255.0
    image_tensor = image_tensor.unsqueeze(0)
    
    return image_tensor, original_size

def postprocess_prediction(prediction, original_size):
    """åå¤„ç†é¢„æµ‹ç»“æœ"""
    # è°ƒæ•´å›åŸå§‹å°ºå¯¸
    prediction_resized = cv2.resize(
        prediction.astype(np.uint8), 
        original_size, 
        interpolation=cv2.INTER_NEAREST
    )
    return prediction_resized

def create_colored_mask(prediction, class_colors):
    """åˆ›å»ºå½©è‰²åˆ†å‰²æ©ç """
    h, w = prediction.shape
    colored_mask = np.zeros((h, w, 3), dtype=np.uint8)
    
    for class_id, color in class_colors.items():
        mask = prediction == class_id
        colored_mask[mask] = color
    
    return colored_mask

def create_overlay_visualization(original_image, colored_mask):
    """åˆ›å»ºå åŠ å¯è§†åŒ–å›¾åƒ"""
    # åˆ›å»ºå åŠ å›¾åƒï¼ˆåŸå›¾+æ©ç ï¼‰
    overlay = cv2.addWeighted(original_image, OVERLAY_ALPHA, colored_mask, MASK_ALPHA, 0)
    return overlay

def create_side_by_side_visualization(original_image, colored_mask, overlay):
    """åˆ›å»ºå¹¶æ’å¯¹æ¯”å›¾åƒ"""
    h, w = original_image.shape[:2]
    
    # åˆ›å»ºä¸‰è”å›¾ï¼šåŸå›¾ | æ©ç  | å åŠ 
    combined = np.zeros((h, w * 3, 3), dtype=np.uint8)
    combined[:, 0:w] = original_image
    combined[:, w:2*w] = colored_mask
    combined[:, 2*w:3*w] = overlay
    
    # æ·»åŠ åˆ†å‰²çº¿
    cv2.line(combined, (w, 0), (w, h), (255, 255, 255), 2)
    cv2.line(combined, (2*w, 0), (2*w, h), (255, 255, 255), 2)
    
    # æ·»åŠ æ ‡ç­¾
    font = cv2.FONT_HERSHEY_SIMPLEX
    font_scale = 1.5
    thickness = 2
    color = (255, 255, 255)
    
    cv2.putText(combined, 'Original', (10, 30), font, font_scale, color, thickness)
    cv2.putText(combined, 'Segmentation', (w + 10, 30), font, font_scale, color, thickness)
    cv2.putText(combined, 'Overlay', (2*w + 10, 30), font, font_scale, color, thickness)
    
    return combined

def analyze_segmentation(prediction):
    """åˆ†æåˆ†å‰²ç»“æœ"""
    unique, counts = np.unique(prediction, return_counts=True)
    total_pixels = prediction.size
    
    analysis = {}
    for class_id, count in zip(unique, counts):
        class_name = ID_TO_CLASS.get(class_id, f"unknown_{class_id}")
        percentage = (count / total_pixels) * 100
        analysis[class_name] = {
            'pixels': int(count),
            'percentage': percentage
        }
    
    return analysis

def save_analysis_report(image_name, analysis, output_dir):
    """ä¿å­˜å•å¼ å›¾ç‰‡çš„åˆ†ææŠ¥å‘Š"""
    report_path = os.path.join(output_dir, f"{image_name}_analysis.txt")
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(f"PCBç„Šç‚¹æ£€æµ‹åˆ†ææŠ¥å‘Š\n")
        f.write(f"å›¾ç‰‡: {image_name}\n")
        f.write("=" * 50 + "\n\n")
        
        # æŒ‰é‡è¦æ€§æ’åºæ˜¾ç¤º
        important_classes = ['good', 'insufficient', 'excess', 'shift', 'background']
        
        for class_name in important_classes:
            if class_name in analysis:
                stats = analysis[class_name]
                f.write(f"{class_name:12s}: {stats['pixels']:8d} åƒç´  ({stats['percentage']:6.2f}%)\n")
        
        # æ˜¾ç¤ºå…¶ä»–ç±»åˆ«
        for class_name, stats in analysis.items():
            if class_name not in important_classes:
                f.write(f"{class_name:12s}: {stats['pixels']:8d} åƒç´  ({stats['percentage']:6.2f}%)\n")
        
        f.write(f"\næ€»åƒç´ æ•°: {sum(stats['pixels'] for stats in analysis.values())}\n")

def create_legend_image():
    """åˆ›å»ºç±»åˆ«é¢œè‰²å›¾ä¾‹"""
    legend_height = 200
    legend_width = 300
    legend = np.ones((legend_height, legend_width, 3), dtype=np.uint8) * 255
    
    font = cv2.FONT_HERSHEY_SIMPLEX
    font_scale = 0.7
    thickness = 2
    
    y_start = 30
    y_step = 35
    
    cv2.putText(legend, 'Class Legend:', (10, 20), font, 0.8, (0, 0, 0), 2)
    
    for i, (class_id, color) in enumerate(CLASS_COLORS.items()):
        if class_id == 0:  # è·³è¿‡èƒŒæ™¯
            continue
            
        class_name = ID_TO_CLASS[class_id]
        y_pos = y_start + i * y_step
        
        # ç»˜åˆ¶é¢œè‰²å—
        cv2.rectangle(legend, (10, y_pos - 15), (40, y_pos + 5), color, -1)
        cv2.rectangle(legend, (10, y_pos - 15), (40, y_pos + 5), (0, 0, 0), 1)
        
        # ç»˜åˆ¶ç±»åˆ«åç§°
        cv2.putText(legend, class_name, (50, y_pos), font, font_scale, (0, 0, 0), thickness)
    
    return legend

def run_inference():
    """è¿è¡Œæ¨ç†ä¸»ç¨‹åº"""
    print("ğŸ”¬ PCBç„Šç‚¹æ£€æµ‹æ¨ç†å·¥å…·")
    print("=" * 60)
    print(f"è®¾å¤‡: {DEVICE}")
    print(f"æ¨¡å‹è·¯å¾„: {MODEL_PATH}")
    print(f"è¾“å…¥ç›®å½•: {INPUT_DIR}")
    print(f"è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
    
    # æ£€æŸ¥è·¯å¾„
    if not os.path.exists(MODEL_PATH):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {MODEL_PATH}")
        return
    
    if not os.path.exists(INPUT_DIR):
        print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {INPUT_DIR}")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # åŠ è½½æ¨¡å‹
    try:
        model, config = load_model_with_config()
        
        # åŠ è½½æƒé‡
        print(f"ğŸ“¦ åŠ è½½æ¨¡å‹æƒé‡...")
        checkpoint = torch.load(MODEL_PATH, map_location=DEVICE)
        model.load_state_dict(checkpoint)
        model = model.to(DEVICE)
        model.eval()
        print("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return
    
    # è·å–æµ‹è¯•å›¾ç‰‡
    image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff')
    test_images = [f for f in os.listdir(INPUT_DIR) 
                   if f.lower().endswith(image_extensions)]
    
    if not test_images:
        print(f"âŒ åœ¨ {INPUT_DIR} ä¸­æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶")
        return
    
    print(f"âœ“ æ‰¾åˆ° {len(test_images)} å¼ å›¾ç‰‡")
    
    # åˆ›å»ºå›¾ä¾‹
    legend = create_legend_image()
    cv2.imwrite(os.path.join(OUTPUT_DIR, "class_legend.jpg"), legend)
    
    # å¤„ç†å›¾ç‰‡
    successful_count = 0
    failed_count = 0
    
    for img_name in tqdm(test_images, desc="å¤„ç†è¿›åº¦"):
        try:
            img_path = os.path.join(INPUT_DIR, img_name)
            base_name = os.path.splitext(img_name)[0]
            
            # è¯»å–å›¾ç‰‡
            original_image = cv2.imread(img_path)
            if original_image is None:
                print(f"âš ï¸ æ— æ³•è¯»å–å›¾ç‰‡: {img_name}")
                failed_count += 1
                continue
            
            # é¢„å¤„ç†
            input_tensor, original_size = preprocess_image(original_image, TARGET_SIZE)
            input_tensor = input_tensor.to(DEVICE)
            
            # æ¨¡å‹æ¨ç†
            with torch.no_grad():
                outputs = model(input_tensor)
                predictions = torch.argmax(outputs, dim=1)
                prediction = predictions.cpu().numpy()[0]
            
            # åå¤„ç†
            prediction = postprocess_prediction(prediction, original_size)
            
            # åˆ›å»ºå¯è§†åŒ–
            colored_mask = create_colored_mask(prediction, CLASS_COLORS)
            overlay = create_overlay_visualization(original_image, colored_mask)
            side_by_side = create_side_by_side_visualization(original_image, colored_mask, overlay)
            
            # ä¿å­˜ç»“æœ
            cv2.imwrite(os.path.join(OUTPUT_DIR, f"{base_name}_original.jpg"), original_image)
            cv2.imwrite(os.path.join(OUTPUT_DIR, f"{base_name}_mask.jpg"), colored_mask)
            cv2.imwrite(os.path.join(OUTPUT_DIR, f"{base_name}_overlay.jpg"), overlay)
            cv2.imwrite(os.path.join(OUTPUT_DIR, f"{base_name}_comparison.jpg"), side_by_side)
            
            # ä¿å­˜åŸå§‹é¢„æµ‹æ©ç ï¼ˆç”¨äºè¿›ä¸€æ­¥åˆ†æï¼‰
            cv2.imwrite(os.path.join(OUTPUT_DIR, f"{base_name}_raw_mask.png"), prediction)
            
            # åˆ†æç»“æœ
            analysis = analyze_segmentation(prediction)
            save_analysis_report(base_name, analysis, OUTPUT_DIR)
            
            successful_count += 1
            
        except Exception as e:
            print(f"âš ï¸ å¤„ç†å›¾ç‰‡ {img_name} æ—¶å‡ºé”™: {e}")
            failed_count += 1
            continue
    
    # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    generate_summary_report(OUTPUT_DIR, successful_count, failed_count, len(test_images), config)
    
    print(f"\nâœ… æ¨ç†å®Œæˆ!")
    print(f"âœ“ æˆåŠŸå¤„ç†: {successful_count} å¼ ")
    print(f"âœ— å¤±è´¥: {failed_count} å¼ ")
    print(f"ğŸ“‚ ç»“æœä¿å­˜åœ¨: {OUTPUT_DIR}")

def generate_summary_report(output_dir, successful, failed, total, config):
    """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
    report_path = os.path.join(output_dir, "inference_summary.txt")
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("PCBç„Šç‚¹æ£€æµ‹æ¨ç†æ€»ç»“æŠ¥å‘Š\n")
        f.write("=" * 60 + "\n\n")
        f.write(f"æ¨ç†æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æ¨¡å‹é…ç½®: {MODEL_CONFIG}\n")
        f.write(f"æ¨¡å‹æ¶æ„: {config['model_name']}\n")
        f.write(f"ç¼–ç å™¨: {config['encoder_name']}\n")
        f.write(f"æè¿°: {config['description']}\n\n")
        
        f.write(f"å¤„ç†ç»Ÿè®¡:\n")
        f.write(f"  æ€»å›¾ç‰‡æ•°: {total}\n")
        f.write(f"  æˆåŠŸå¤„ç†: {successful}\n")
        f.write(f"  å¤„ç†å¤±è´¥: {failed}\n")
        f.write(f"  æˆåŠŸç‡: {successful/total*100:.1f}%\n\n")
        
        f.write(f"è¾“å…¥ç›®å½•: {INPUT_DIR}\n")
        f.write(f"è¾“å‡ºç›®å½•: {output_dir}\n")
        f.write(f"æ¨¡å‹è·¯å¾„: {MODEL_PATH}\n\n")
        
        f.write("ç±»åˆ«è¯´æ˜:\n")
        for class_name, class_id in CLASS_MAPPING.items():
            color = CLASS_COLORS[class_id]
            f.write(f"  {class_name:12s} (ID: {class_id}) - BGR{color}\n")
        
        f.write("\nç”Ÿæˆçš„æ–‡ä»¶ç±»å‹:\n")
        f.write("  *_original.jpg     - åŸå§‹å›¾ç‰‡\n")
        f.write("  *_mask.jpg         - å½©è‰²åˆ†å‰²æ©ç \n")
        f.write("  *_overlay.jpg      - å åŠ å¯è§†åŒ–\n")
        f.write("  *_comparison.jpg   - ä¸‰è”å¯¹æ¯”å›¾\n")
        f.write("  *_raw_mask.png     - åŸå§‹é¢„æµ‹æ©ç \n")
        f.write("  *_analysis.txt     - åƒç´ ç»Ÿè®¡åˆ†æ\n")
        f.write("  class_legend.jpg   - ç±»åˆ«é¢œè‰²å›¾ä¾‹\n")
        f.write("  inference_summary.txt - æœ¬æ€»ç»“æŠ¥å‘Š\n")

if __name__ == "__main__":
    run_inference()
